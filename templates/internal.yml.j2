Parameter_defaults:
  LocalCephAnsibleFetchDirectoryBackup: /tmp/fetch_dir
  CephPoolDefaultSize: 1
  # when deploying a small number of osd's - < 12), it's necessary to decrease the default pg_num from 128 to get past the max 200pgs/per osd  limitation
  CephPoolDefaultPgNum: 32
  CephAnsiblePlaybookVerbosity: 1
  CephAnsibleDisksConfig:
    devices:
      - /dev/nvme0n1
# the following two parameters are the defaults. Just included them here for info
    osd_scenario: lvm
    osd_objectstore: bluestore
  CephAnsibleExtraConfig:
    osd_pool_default_autoscale_mode: on

